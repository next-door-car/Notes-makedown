### **【2023】Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks**

-  [github复现] ( https://github.com/cbfinn/maml )
![图片](D:/Notes/深度学习/无监督学习/pictures/2/1.png)

### 综述
- 数据集
  - **Omniglot**  数据集
  - **MiniImagenet**  数据集
- 方法
  - 使用** TensorFlow** 架构
  - **MAML**(元学习模型)权重使用梯度更新
  - 随机梯度下降**(SGD)**进行跨任务的元优化
  - **A Model-Agnostic Meta-Learning Algorithm**  
    - 一种与模型无关的元学习算法（基于梯度的学习规则）

### 研究
#### 提出难点
  - 如何通过各种学习任务上训练的一个模型，使只用少量的训练样本就可以解决新的学习任务
  - 对 K-shot 学习和快速强化学习等问题进行极其有效的适应
#### 分析重点
  - θ ---> θi ,损失L
<div style="display: flex; justify-content: center;">
    <img src="D:/Notes/深度学习/无监督学习/pictures/2/2.png" alt="图片描述" />
</div>

  - Few-shot学习
    - 从中学习一个新函数，只有几个输入/输出对，使用来自类似任务的先验数据进行元学习
    - 使用均方误差(MSE)
![图片](D:/Notes/深度学习/无监督学习/pictures/2/3.png)

    - 使用交叉熵损失
<div style="display: flex; justify-content: center;">
    <img src="D:/Notes/深度学习/无监督学习/pictures/2/4.png" alt="图片描述" />
</div>

  - 基于 K-shot 强化学习( RL)
    - ......
  
#### 解决方案
1. 进行参数的调整
  - 由参数化函数 f(θ) 表示的模型，参数为 θ。当适应新的任务 Ti 时，模型参数 θ 变为 θi
  - 任务 Ti 上的一个或多个梯度下降更新来计算更新后的参数向量 θi
    - 使用一个梯度更新时参数关系
<div style="display: flex; justify-content: center;">
    <img src="D:/Notes/深度学习/无监督学习/pictures/2/5.png" alt="图片描述" />
</div>

    - 使用随机梯度下降(SGD)进行跨任务的元优化，β 是元步长
<div style="display: flex; justify-content: center;">
    <img src="D:/Notes/深度学习/无监督学习/pictures/2/6.png" alt="图片描述" />
</div>

#### 验证结果
1. 使用正选波的输入输出回归验证MAML的性能 梯度更新K
  - 由一半的振幅相位推断另一半的振幅相位
![图片](D:/Notes/深度学习/无监督学习/pictures/2/7.png)

![图片](D:/Notes/深度学习/无监督学习/pictures/2/8.png)

2. 在 Omniglot 和 MiniImagenet 数据集上进行少量图像识别。与 memory-augmented neural networks  对比
  - 实验方案 N-way 分类：
    - Omniglot 选择1200个字符训练并使用剩余的用于测试
<div style="display: flex; justify-content: center;">
    <img src="D:/Notes/深度学习/无监督学习/pictures/2/9.png" alt="图片描述" />
</div>

<div style="height: 20px;"></div>

![图片](D:/Notes/深度学习/无监督学习/pictures/2/10.png)

### Tips
  - 基于梯度下降过程训练
    - 目的是训练模型的参数
  - 在 元训练过程中，从p(T)中采样一个任务Ti，使用来自Ti的K个样本和相应损失LTi的反馈来训练模型 ，然后在来自Ti的新样本 上进行测试
  - 目标是找到 对任务变化敏感的模型参数
  - 提出的模型方法旨在优化模型参数
  - MAML学习器的权重使用梯度更新
  - 可以视为显式地最大化新任务损失对模型参数的敏感性。
  
#### 优点
  - 不会扩展学习参数的数量，也不会对模型架构施加约束
  - 可以很容易地与全连接、卷积或循环神经网络结合
  - 适用于使用小型 数据集进行快速训练
  
#### 缺点(局限性)