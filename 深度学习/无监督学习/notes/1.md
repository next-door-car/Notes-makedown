### **【2023】Revisiting Reverse Distillation for Anomaly Detection**

-  [github复现]( https://github.com/tientrandinh/ Revisiting-Reverse-Distillation)
![图片](D:/Notes/深度学习/无监督学习/pictures/1/1.jpg)

### 综述 
- 数据集
  - MVTec 数据集
  - BTAD  数据集
  - Retinal OCT  数据集
- 方法
  - 传统方法
    - RD (反向蒸馏)
    - PatchCore  
    - Coupled-hyperspherebased Feature Adaptation (CFA)   
### 研究
提出难点
  - 精度低，速度慢
  - 仅依赖蒸馏任务和一个OCBE（one class bottleneck embedding）模块无法为学生提供紧凑的表示
    - OCBE 模块没有一个目标函数来防止异常信息传递给学生，不能保证异常模式在对异常样本进行推断时不会大量流向学生
分析重点
  - PatchCore   存储图像子采样核心集的方法
  - CFA 通过度量学习获得判别特征的方法 （通过内存库实现）
    - 需要创建内存库，这需要高成本的计算和复杂的架构。
  - RD 框架遵循编码器-解码器架构  从后向前处理各层
  - RD++ 在RD 基础框架下如何严格防止OCBE模块接收异常模式
### 解决方案
1.在RD基础上加入投影层，将蒸馏损失（KD）与其他损失函数结合(将教师网络中各自模块后面的投影层集成在一起)
        T-S 模型以余弦相似度损失作为蒸馏损失
<div style="display: flex; justify-content: center;">
    <img src="D:/Notes/深度学习/无监督学习/pictures/1/2.png" alt="图片描述" />
</div>
<div style="height: 20px;"></div>
![图片](D:/Notes/深度学习/无监督学习/pictures/1/3.png)
2.损失如下

  - *LKD : 余弦相似度损失*

  - *LSSOT ：自监督最优运输损失*

![图片](D:/Notes/深度学习/无监督学习/pictures/1/4.png)

  - *LRecon ：重建损失*

![图片](D:/Notes/深度学习/无监督学习/pictures/1/5.png)

  - *LCon ：对比度损失*

![图片](D:/Notes/深度学习/无监督学习/pictures/1/6.png)

    - L = LKD + αLSSOT + βLRecon + γLCon （总损失）
    - 上述α， β 和 γ 是正正则化参数
  3.RD++推理程序

![图片](D:/Notes/深度学习/无监督学习/pictures/1/7.png)

  4.噪声选取
  - 使用单纯形噪声（比高斯白噪声更自然产生异常模式）

![图片](D:/Notes/深度学习/无监督学习/pictures/1/8.png)
    
### 验证结果
1. 256*256  WideResNet50  作为 backbone  
2. 使用Adam Optimizer 学生、OCBE模块的学习速率设置为0.005，0.001 为投影层。蒸馏损失、SSOT 损失、对比损失和重构损失的权重设为1，0.2，0.02和0.002。
3. 训练数据包含3629张正常图像。测试集包含1725个样本，其中既有正常的 图像，也有异常的图像。
<div style="height: 10px;"></div>

![图片](D:/Notes/深度学习/无监督学习/pictures/1/9.png)

<div style="height: 10px;"></div>

![图片](D:/Notes/深度学习/无监督学习/pictures/1/10.png)

<div style="height: 10px;"></div>

![图片](D:/Notes/深度学习/无监督学习/pictures/1/11.png)

### Tips
  - RD++本质是对RD的架构和损失函数的若干修改，基于反蒸馏(RD)思想的启发而来
#### 优点
  - 推理仅需4G内存
  - 速度快，低延时性
  - AUROC指标99.73%，高于最近所有的最先进技术
#### 缺点  (局限性)