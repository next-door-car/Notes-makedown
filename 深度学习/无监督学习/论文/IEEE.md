# Revisiting Reverse Distillation for Anomaly Detection

逆向蒸馏(RD)
师生网络对，知识 从教师传递给学生。
教师的输出通过一个类瓶颈模块(OCBE)提供给学生。反蒸馏方法实现 具有竞争力的性能，并保持低延迟

RD与 多任务学习结合起来，提出了RD++\


提出难点
  - 精度低，速度慢
  - 仅依赖蒸馏任务和一个OCBE（one class bottleneck embedding）模块无法为学生提供紧凑的表示
分析重点
  - PatchCore   存储图像子采样核心集的方法
  - CFA 通过度量学习获得判别特征的方法 （通过内存库实现）
    - 需要创建内存库，这需要高成本的计算和复杂的架构。
  - RD 框架遵循编码器-解码器架构  从后向前处理各层
  


RD 包含三个模块:一个固定的预训练教师作为 编码器，一个可训练的单类嵌入(OCBE)模块， 和一个学生作为解码器OCBE模块采用Resnet的最后一个块[13]进行特征提取。提出了通过将模式压缩到低维 空间中，并去噪来增强异常差异的方法




  培训期间我们的rdr++概述。首先，我们在每个中间教师块之后直接集成一个投影层，以 为学生网络提供一个紧凑的、无异常的表示。将[8]中引入的蒸馏损失(LKD)与其他 多重损失函数相结合进行优化。对于特征紧凑性任务，提出了两种损失函数:自监督最优 传输损失(LSSOT)，用于将正态特征空间投影到紧凑表示;对比度损失(LCon)支持投影层 通过设置投影归一法学习紧凑嵌入